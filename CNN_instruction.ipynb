{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to construct the CNN #\n",
    "# ------------------------ #\n",
    "# In general, xdata represents images while ydata represents labels. #\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import cv2\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Load data\n",
    "train_dir = '.\\\\images\\\\doge.jpg'\n",
    "test_dir = ''\n",
    "train_x = np.asarray([cv2.imread(train_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant paramerters of this CNN model\n",
    "\n",
    "input -> 100C3-MP2 -> 200C2-MP2 -> 300C2-MP2 -> 400C2-MP2 -> 500C2-MP2 -> 600C2-MP2 -> 700C2 -> output\n",
    "\n",
    "The 7th convolutional layer is directly connected with the fully connected layer which leads the output of this CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4913f54a76bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mim_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mim_height\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim_height\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "# Set model parameters\n",
    "batch_size = 1\n",
    "learning_rate = 0.005\n",
    "test_size = 20\n",
    "im_width = train_x[0].shape[0]\n",
    "im_height = train_x[0].shape[1]\n",
    "print(im_height)\n",
    "labels_size = 10\n",
    "num_channels = 3\n",
    "train_epochs = 500\n",
    "\n",
    "conv1_output = 100\n",
    "conv2_output = 200\n",
    "conv3_output = 300\n",
    "conv4_output = 400\n",
    "conv5_output = 500\n",
    "conv6_output = 600\n",
    "conv7_output = 700\n",
    "conv_size = 2\n",
    "\n",
    "max_pool_size1 = 2\n",
    "max_pool_size2 = 2\n",
    "max_pool_size3 = 2\n",
    "max_pool_size4 = 2\n",
    "max_pool_size5 = 2\n",
    "max_pool_size6 = 2\n",
    "\n",
    "fully_connected1_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare tensor variables\n",
    "\n",
    "The issue should be empasized here is that, for convolutional layer, the shape of **_kernal_** has 4 dimensions which are \n",
    "```python\n",
    "[filter_height * filter_width * in_channels, output_channels]\n",
    "```\n",
    "\n",
    "while the **_input tensor_** of convolutional layer is restricted by\n",
    "\n",
    "```python\n",
    "[batch, in_height, in_width, in_channels]\n",
    "```\n",
    "The input channel of current layer equals to the output channel of previous one. Following is the declaration of convolutional kernal.\n",
    "```python\n",
    "conv1_weight = weight_variable([3, 3, num_channels, conv1_output])\n",
    "conv2_weight = weight_variable([conv_size, conv_size, conv1_output, conv2_output])\n",
    "conv3_weight = weight_variable([conv_size, conv_size, conv2_output, conv3_output])\n",
    "...\n",
    "```\n",
    "\n",
    "### Output dimension after each layer\n",
    "\n",
    "```python\n",
    "max_pool1: (10, 128, 128, 100)\n",
    "max_pool2: (10, 64, 64, 200)\n",
    "max_pool3: (10, 32, 32, 300)\n",
    "max_pool4: (10, 16, 16, 400)\n",
    "max_pool5: (10, 8, 8, 500)\n",
    "max_pool6: (10, 4, 4, 600)\n",
    "relu7: (10, 4, 4, 700)\n",
    "```\n",
    "From the results above we can see that the input size of fully connected layer after those convolutional layer will finally be `(10, 4, 4, 700)`. Note that, each time max pool layer applied, the dimension will reduce related the kernel size in pool layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare model placeholders\n",
    "x_input_shape = (batch_size, im_width, im_height, num_channels)\n",
    "x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "y_label = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1, dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.zeros(shape = shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Convolution variables\n",
    "conv1_weight = weight_variable([3, 3, num_channels, conv1_output])\n",
    "conv1_bias = bias_variable([conv1_output])\n",
    "\n",
    "conv2_weight = weight_variable([conv_size, conv_size, conv1_output, conv2_output])\n",
    "conv2_bias = bias_variable([conv2_output])\n",
    "\n",
    "conv3_weight = weight_variable([conv_size, conv_size, conv2_output, conv3_output])\n",
    "conv3_bias = bias_variable([conv3_output])\n",
    "\n",
    "conv4_weight = weight_variable([conv_size, conv_size, conv3_output, conv4_output])\n",
    "conv4_bias = bias_variable([conv4_output])\n",
    "\n",
    "conv5_weight = weight_variable([conv_size, conv_size, conv4_output, conv5_output])\n",
    "conv5_bias = bias_variable([conv5_output])\n",
    "\n",
    "conv6_weight = weight_variable([conv_size, conv_size, conv5_output, conv6_output])\n",
    "conv6_bias = bias_variable([conv6_output])\n",
    "\n",
    "conv7_weight = weight_variable([conv_size, conv_size, conv6_output, conv7_output])\n",
    "conv7_bias = bias_variable([conv7_output])\n",
    "\n",
    "# Fully connected variables\n",
    "resulting_width = im_width // (max_pool_size1 * max_pool_size2 * max_pool_size3 * max_pool_size4 * max_pool_size5 * max_pool_size6)\n",
    "resulting_height = im_height // (max_pool_size1 * max_pool_size2 * max_pool_size3 * max_pool_size4 * max_pool_size5 * max_pool_size6)\n",
    "full1_input_size = resulting_height * resulting_width * conv7_output\n",
    "full1_weight = weight_variable([full1_input_size, fully_connected1_size])\n",
    "full1_bias = weight_variable([fully_connected1_size])\n",
    "full2_weight = weight_variable([fully_connected1_size, labels_size])\n",
    "full2_bias = weight_variable([labels_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize Model Operations\n",
    "def my_CNN(input):\n",
    "\t# 1st layer: 100C3-MP2\n",
    "\tconv1 = tf.nn.conv2d(input, conv1_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\trelu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "\tmax_pool1 = tf.nn.max_pool(relu1, ksize=[1, max_pool_size1, max_pool_size1, 1],\n",
    "\t\t                        strides=[1, max_pool_size1, max_pool_size1, 1], padding='SAME')\n",
    "\tprint('max_pool1:', max_pool1.shape)\n",
    "\n",
    "\t# 2nd layer: 200C2-MP2\n",
    "\tconv2 = tf.nn.conv2d(max_pool1, conv2_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\trelu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_bias))\n",
    "\tmax_pool2 = tf.nn.max_pool(relu2, ksize=[1, max_pool_size2, max_pool_size2, 1],\n",
    "\t\t                        strides=[1, max_pool_size2, max_pool_size2, 1], padding='SAME')\n",
    "\tprint('max_pool2:', max_pool2.shape)\n",
    "\n",
    "\t# 3rd layer: 300C2-MP2\n",
    "\tconv3 = tf.nn.conv2d(max_pool2, conv3_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\trelu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_bias))\n",
    "\tmax_pool3 = tf.nn.max_pool(relu3, ksize=[1, max_pool_size3, max_pool_size3, 1],\n",
    "\t\t                        strides=[1, max_pool_size3, max_pool_size3, 1], padding='SAME')\n",
    "\tprint('max_pool3:', max_pool3.shape)\n",
    "\n",
    "\t# 4th layer: 400C2-MP2\n",
    "\tconv4 = tf.nn.conv2d(max_pool3, conv4_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\trelu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_bias))\n",
    "\tmax_pool4 = tf.nn.max_pool(relu4, ksize=[1, max_pool_size4, max_pool_size4, 1],\n",
    "\t\t                        strides=[1, max_pool_size4, max_pool_size4, 1], padding='SAME')\n",
    "\tprint('max_pool4:', max_pool4.shape)\n",
    "\n",
    "\t# 5th layer: 500C2-MP2\n",
    "\tconv5 = tf.nn.conv2d(max_pool4, conv5_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\trelu5 = tf.nn.relu(tf.nn.bias_add(conv5, conv5_bias))\n",
    "\tmax_pool5 = tf.nn.max_pool(relu5, ksize=[1, max_pool_size5, max_pool_size5, 1],\n",
    "\t\t                        strides=[1, max_pool_size5, max_pool_size5, 1], padding='SAME')\n",
    "\tprint('max_pool5:', max_pool5.shape)\n",
    "\n",
    "\t# 6th layer: 600C2-MP2\n",
    "\tconv6 = tf.nn.conv2d(max_pool5, conv6_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\trelu6 = tf.nn.relu(tf.nn.bias_add(conv6, conv6_bias))\n",
    "\tmax_pool6 = tf.nn.max_pool(relu6, ksize=[1, max_pool_size6, max_pool_size6, 1],\n",
    "\t\t                        strides=[1, max_pool_size6, max_pool_size6, 1], padding='SAME')\n",
    "\tprint('max_pool6:', max_pool6.shape)\n",
    "\n",
    "\t# 7th layer: 700C2\n",
    "\tconv7 = tf.nn.conv2d(max_pool6, conv7_weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\trelu7 = tf.nn.relu(tf.nn.bias_add(conv7, conv7_bias))\n",
    "\tprint('relu7:', relu7.shape)\n",
    "\n",
    "\t# Flat the output from conv layers for next fully connected layers\n",
    "\tfinal_conv_shape = relu7.get_shape().as_list()\n",
    "\tflat_shape = final_conv_shape[1] * final_conv_shape[2] * final_conv_shape[3]\n",
    "\tflat_output = tf.reshape(relu7, [final_conv_shape[0], flat_shape])\n",
    "\n",
    "\t# fully connected layer\n",
    "\t#fully_connected1 = tf.nn.relu(tf.add)\n",
    "\n",
    "model_output = my_CNN(x_input)\n",
    "\n",
    "# Declare Loss function (softmax cross entropy)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_label))\n",
    "\n",
    "# Creat a prediction function\n",
    "prediction = tf.nn.softmax(model_output)\n",
    "\n",
    "# Create an optimizer\n",
    "my_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "# Calculate accuracy function\n",
    "# In this function, batch_prediction is the ouput result from the CNN\n",
    "# while labels are the real label stored in dataset which trains the model\n",
    "def get_acc(logists, labels):\n",
    "\tbatch_predictions = np.argmax(logists, axis=1)\n",
    "\tbingo = np.sum(np.equal(batch_predictions, labels))\n",
    "\treturn(100. * bingo/batch_predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For training\n",
    "\n",
    "So far, we have constructed the CNN model as expexted while another crucial part remains as problem which is the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(sess.run(prediction, {x_input: train_x}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
