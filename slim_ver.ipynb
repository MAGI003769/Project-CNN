{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to construct the CNN #\n",
    "# ------------------------ #\n",
    "# In general, xdata represents images while ydata represents labels. #\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import cv2\n",
    "import tensorflow.contrib.slim as slim\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Load data\n",
    "train_dir = '.\\\\images\\\\doge.jpg'\n",
    "test_dir = ''\n",
    "train_x = np.asarray([cv2.imread(train_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant paramerters of this CNN model\n",
    "input -> 100C3-MP2 -> 200C2-MP2 -> 300C2-MP2 -> 400C2-MP2 -> 500C2-MP2 -> 600C2-MP2 -> 700C2 -> output\n",
    "The 7th convolutional layer is directly connected with the fully connected layer which leads the output of this CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set model parameters\n",
    "batch_size = 1\n",
    "learning_rate = 0.005\n",
    "test_size = 20\n",
    "im_width = train_x[0].shape[0]\n",
    "im_height = train_x[0].shape[1]\n",
    "labels_size = 10\n",
    "num_channels = 3\n",
    "train_epochs = 500\n",
    "keep_prob = 1\n",
    "\n",
    "conv1_output = 100\n",
    "conv2_output = 200\n",
    "conv3_output = 300\n",
    "conv4_output = 400\n",
    "conv5_output = 500\n",
    "conv6_output = 600\n",
    "conv7_output = 700\n",
    "conv_size = 2\n",
    "\n",
    "max_pool_size1 = 2\n",
    "max_pool_size2 = 2\n",
    "max_pool_size3 = 2\n",
    "max_pool_size4 = 2\n",
    "max_pool_size5 = 2\n",
    "max_pool_size6 = 2\n",
    "\n",
    "fully_connected1_size = 100\n",
    "\n",
    "# Declare model placeholders\n",
    "x_input_shape = (batch_size, im_width, im_height, num_channels)\n",
    "x_input = tf.placeholder(tf.float32, shape=x_input_shape)\n",
    "y_label = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1, dtype=tf.float32)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.zeros(shape = shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare tensor variables\n",
    "\n",
    "The issue should be empasized here is that, for convolutional layer, the shape of **_kernal_** has 4 dimensions which are \n",
    "```python\n",
    "[filter_height, filter_width, in_channels, out_channels]\n",
    "```\n",
    "\n",
    "while the **_input tensor_** of convolutional layer is restricted by\n",
    "\n",
    "```python\n",
    "[batch, in_height, in_width, in_channels]\n",
    "```\n",
    "The input channel of current layer equals to the output channel of previous one. Following is the declaration of convolutional kernal.\n",
    "```python\n",
    "conv1_weight = weight_variable([3, 3, num_channels, conv1_output])\n",
    "conv2_weight = weight_variable([conv_size, conv_size, conv1_output, conv2_output])\n",
    "conv3_weight = weight_variable([conv_size, conv_size, conv2_output, conv3_output])\n",
    "...\n",
    "```\n",
    "\n",
    "### Output dimension after each layer\n",
    "\n",
    "```python\n",
    "max_pool1: (10, 128, 128, 100)\n",
    "max_pool2: (10, 64, 64, 200)\n",
    "max_pool3: (10, 32, 32, 300)\n",
    "max_pool4: (10, 16, 16, 400)\n",
    "max_pool5: (10, 8, 8, 500)\n",
    "max_pool6: (10, 4, 4, 600)\n",
    "relu7: (10, 4, 4, 700)\n",
    "```\n",
    "From the results above we can see that the input size of fully connected layer after those convolutional layer will finally be `(10, 4, 4, 700)`. Note that, each time max pool layer applied, the dimension will reduce related the kernel size in pool layer.\n",
    "\n",
    "### Parameters interpretation\n",
    "\n",
    "- [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)<br>\n",
    "    This function seems to only execute convolution calculation which does not include the activation of neural units in networks as there is no parameter to specify activation function.\n",
    "<br>\n",
    "- [slim.conv2d](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d)<br>\n",
    "    [*_Slim_*](https://github.com/tensorflow/tensorflow/tree/r1.2/tensorflow/contrib/slim) performs as a interface that directly call a function to construct a convolutional layer that include activation function.\n",
    "\n",
    "\n",
    "The use of `slim.conv2d` will lead a much more clear graph structure in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0177173\n",
      "0.0625412\n",
      "0.0079451\n",
      "0.0780047\n",
      "0.380059\n",
      "0.0393354\n",
      "0.00784571\n",
      "0.0130283\n",
      "0.0228492\n",
      "0.370675\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model Operations\n",
    "def my_CNN(input):\n",
    "    # 1st layer: 100C3-MP2\n",
    "    conv_1 = slim.conv2d(input, 100, [3, 3], 1, padding='SAME', scope='conv1',activation_fn=tf.nn.relu)\n",
    "    max_pool1 = slim.max_pool2d(conv_1, [2, 2], [2, 2], padding='SAME')\n",
    "\n",
    "    # 2nd layer: 200C2-MP2\n",
    "    conv_2 = slim.conv2d(max_pool1, 200, [2, 2], 1, padding='SAME', scope='conv2',activation_fn=tf.nn.relu)\n",
    "    max_pool2 = max_pool_1 = slim.max_pool2d(conv_2, [2, 2], [2, 2], padding='SAME')\n",
    "\n",
    "    # 3rd layer: 300C2-MP2\n",
    "    conv_3 = slim.conv2d(max_pool2, 300, [2, 2], 1, padding='SAME', scope='conv3',activation_fn=tf.nn.relu)\n",
    "    max_pool3 = max_pool_1 = slim.max_pool2d(conv_3, [2, 2], [2, 2], padding='SAME')\n",
    "\n",
    "    # 4th layer: 400C2-MP2\n",
    "    conv_4 = slim.conv2d(max_pool3, 400, [2, 2], 1, padding='SAME', scope='conv4',activation_fn=tf.nn.relu)\n",
    "    max_pool4 = max_pool_1 = slim.max_pool2d(conv_4, [2, 2], [2, 2], padding='SAME')\n",
    "    \n",
    "    # 5th layer: 500C2-MP2\n",
    "    conv_5 = slim.conv2d(max_pool4, 500, [2, 2], 1, padding='SAME', scope='conv5',activation_fn=tf.nn.relu)\n",
    "    max_pool5 = max_pool_1 = slim.max_pool2d(conv_5, [2, 2], [2, 2], padding='SAME')\n",
    "\n",
    "    # 6th layer: 600C2-MP2\n",
    "    conv_6 = slim.conv2d(max_pool5, 600, [2, 2], 1, padding='SAME', scope='conv6',activation_fn=tf.nn.relu)\n",
    "    max_pool6 = max_pool_1 = slim.max_pool2d(conv_6, [2, 2], [2, 2], padding='SAME')\n",
    "\n",
    "    # 7th layer: 700C2\n",
    "    conv_7 = slim.conv2d(max_pool6, 700, [2, 2], 1, padding='SAME', scope='conv7',activation_fn=tf.nn.relu)\n",
    "\n",
    "    # Flat the output from conv layers for next fully connected layers\n",
    "    flatten = slim.flatten(conv_7)\n",
    "    \n",
    "    # 1st fully connected layer\n",
    "    fc1 = slim.fully_connected(slim.dropout(flatten, keep_prob), 1024,\n",
    "                                   activation_fn=tf.nn.tanh, scope='fc1')\n",
    "\n",
    "    # 2nd fully connected layer\n",
    "    model_output = slim.fully_connected(slim.dropout(fc1, keep_prob), labels_size,\n",
    "                                   activation_fn=None, scope='fc2')\n",
    "\n",
    "    return(model_output)\n",
    "\n",
    "model_output = my_CNN(x_input)\n",
    "\n",
    "# Declare Loss function (softmax cross entropy)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model_output, labels=y_label))\n",
    "\n",
    "# Creat a prediction function\n",
    "prediction = tf.nn.softmax(model_output)\n",
    "\n",
    "# Create an optimizer\n",
    "my_optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = my_optimizer.minimize(loss)\n",
    "\n",
    "# Calculate accuracy function\n",
    "# In this function, batch_prediction is the ouput result from the CNN\n",
    "# while labels are the real label stored in dataset which trains the model\n",
    "def get_acc(logits, labels):\n",
    "    batch_predictions = np.argmax(logits, axis=1)\n",
    "    bingo = np.sum(np.equal(batch_predictions, labels))\n",
    "    return(100. * bingo/batch_predictions.shape[0])\n",
    "\n",
    "# Run the model\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    summary = sess.run(prediction, {x_input: train_x})\n",
    "    for i in range(10):\n",
    "        print(summary[0][i])\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "1. Still need further working on input data sets.\n",
    "    - shape, lables\n",
    "    - randmly feeding\n",
    "    - batches\n",
    "<br>\n",
    "<br>\n",
    "2. Training and testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
